import os
import csv
import yaml
from snakemake.io import expand
import pandas as pd
import glob



print("Configuration loaded:", config)


#Parse samples from .csv files
def parse_samples(samples_file):
    samples = {}
    with open(samples_file, mode='r') as infile:
        reader = csv.DictReader(infile)
        for row in reader:
            sample_id = row['ID']
            forward_read = row['forward']
            reverse_read = row['reverse']
            samples[sample_id] = {"R1": forward_read, "R2": reverse_read}
    return samples



#Parse protein references from CSV
def parse_protein_references(protein_reference_file):
    protein_references = {}
    with open(protein_reference_file, mode='r') as infile:
        reader = csv.DictReader(infile)
        for row in reader:
            sample_id = row['process_id']
            reference_name = row['reference_name']
            reference_path = row['reference_path']
            accession_number = row['accession_number']
            protein_references[sample_id] = {"reference_name": reference_name, "reference_path": reference_path}
    return protein_references


print("Loaded config keys:", config.keys())

samples = parse_samples(config["samples_file"])
protein_references = parse_protein_references(config["protein_reference_file"])

output_dir = config["output_dir"]

r = config["r"]
s = config["s"]

run_name = config["run_name"]

mge_path = config["mge_path"]




#Create lists of samples and corresponding references, and link them
sample_list = list(samples.keys())

reference_list = [protein_references[sample]["reference_name"] for sample in sample_list]

sample_reference_pairs = list(zip(sample_list, reference_list))


print(f"Run name: {run_name}")
print(f"Output directory: {output_dir}")
print(f"r params: {r}")
print(f"s params: {s}")
print("reference list:", reference_list)
print("sample reference pairs:", sample_reference_pairs)






#Rule them ALL
rule all:
    input:
        expand(
            os.path.join(output_dir, "consensus/{sample}_r_{r}_s_{s}_con_renamed_{reference_name}.fas"),
            zip,
            sample=[s for s, _ in sample_reference_pairs],
            reference_name=[ref for _, ref in sample_reference_pairs],
            r=config["r"],
            s=config["s"]
        ),
        os.path.join(output_dir, f"consensus/{run_name}.fasta"),
        os.path.join(output_dir, "alignment/alignment_files.log"),
        os.path.join(output_dir, f"{run_name}.csv"), 
        os.path.join(output_dir, "cleanup_complete.txt")





rule fastp_pe:
    input:
        R1=lambda wildcards: samples[wildcards.sample]["R1"],
        R2=lambda wildcards: samples[wildcards.sample]["R2"]
    output:
        R1_trimmed=os.path.join(output_dir, "trimmed_data/{sample}_R1_trimmed.fq.gz"),
        R2_trimmed=os.path.join(output_dir, "trimmed_data/{sample}_R2_trimmed.fq.gz"),
        report=os.path.join(output_dir, "trimmed_data/reports/{sample}_fastp_report.html"),
        json=os.path.join(output_dir, "trimmed_data/reports/{sample}_fastp_report.json"),
        merged_reads=temp(os.path.join(output_dir, "trimmed_data/{sample}_merged.fq")),
        unpaired_R1=temp(os.path.join(output_dir, "trimmed_data/unpaired/{sample}_unpaired_R1.fq")),
        unpaired_R2=temp(os.path.join(output_dir, "trimmed_data/unpaired/{sample}_unpaired_R2.fq"))
    log:
        err=os.path.join(output_dir, "trimmed_data/{sample}_fastp.err")
    shell:
        """
        fastp -i {input.R1} -I {input.R2} \
              -o {output.R1_trimmed} -O {output.R2_trimmed} \
              --adapter_sequence=AGATCGGAAGAGCACACGTCTGAACTCCAGTCA \
              --adapter_sequence_r2=AGATCGGAAGAGCGTCGTGTAGGGAAAGAGTGT \
              --dedup \
              --trim_poly_g \
              --merge --merged_out {output.merged_reads} \
              --unpaired1 {output.unpaired_R1} \
              --unpaired2 {output.unpaired_R2} \
              -h {output.report} -j {output.json} \
              > {log.err}
        """





rule clean_headers:
    input:
        merged_reads=os.path.join(output_dir, "trimmed_data/{sample}_merged.fq")
    output:
        clean_merged=os.path.join(output_dir, "trimmed_data/{sample}_merged_clean.fq")
    shell:
        """
        sed 's/ /_/g' {input.merged_reads} > {output.clean_merged}
        """







rule MitoGeneExtractor_default:
    input:
        DNA=os.path.join(output_dir, "trimmed_data/{sample}_merged_clean.fq"),
        AA=lambda wildcards: protein_references[wildcards.sample]["reference_path"]
    output:
        alignment=os.path.join(output_dir, "alignment/{sample}_r_{r}_s_{s}_align_{reference_name}.fas"),
        consensus=os.path.join(output_dir, "consensus/{sample}_r_{r}_s_{s}_con_{reference_name}.fas"),
        vulgar=os.path.join(output_dir, "logs/{sample}_vulgar_r_{r}_s_{s}_{reference_name}.txt")
    log:
        out=os.path.join(output_dir, "out/{sample}_r_{r}_s_{s}_summary_{reference_name}.out"),
        err=os.path.join(output_dir, "err/{sample}_r_{r}_s_{s}_summary_{reference_name}.err"),
    params:
        mge_executor=config["mge_path"]
    shell:
        """
        #echo input.AA list to log
        echo "Input AA files: {input.AA}" > {log.out}

        {params.mge_executor} \
        -q {input.DNA} -p {input.AA} \
        -o {output_dir}/alignment/{wildcards.sample}_r_{wildcards.r}_s_{wildcards.s}_align_ \
        -c {output_dir}/consensus/{wildcards.sample}_r_{wildcards.r}_s_{wildcards.s}_con_ \
        -V {output.vulgar}  \
        -r {wildcards.r} -s {wildcards.s} \
        -n 0 -C 5 -t 0.5 \
        > {log.out} 2> {log.err}
        """



#Define list of consensus files and renamed consensus files for downstream rules
consensus_files = [
    os.path.join(output_dir, f"consensus/{sample}_r_{r}_s_{s}_con_{reference_name}.fas")
    for sample, reference_name in sample_reference_pairs
    for r in config["r"]
    for s in config["s"]
]
print(f"Consensus files: {consensus_files}")



renamed_files = [
    os.path.join(output_dir, f"consensus/{sample}_r_{r}_s_{s}_con_renamed_{reference_name}.fas")
    for sample, reference_name in sample_reference_pairs
    for r in config["r"]
    for s in config["s"]
]
print(f"Renamed consensus files: {renamed_files}")







#Rename headers in consensus files
rule rename_fasta_headers:
    input:
        consensus_files=consensus_files
    output:
        renamed_consensus_files=renamed_files
    run:
        for input_file, output_file in zip(input.consensus_files, output.renamed_consensus_files):
            #Extract 'sample', 'r', 's' and reference used from filename
            base_name = os.path.basename(input_file)
            sample = base_name.split('_')[0]
            r = base_name.split('_')[2]
            s = base_name.split('_')[4]
            con_suffix = base_name.split('con_')[-1].replace('.fas', '')


            os.makedirs(os.path.dirname(output_file), exist_ok=True)

            #Rename headers to include alt params and reference used for MGE run
            with open(input_file, 'r') as infile, open(output_file, 'w') as outfile:
                for line in infile:
                    if line.startswith('>'):
                        outfile.write(f">{sample}_r_{r}_s_{s}_{con_suffix}_fastp\n")
                    else:
                        outfile.write(line)





#Concatenate all consensus fasta files
rule concatenate_fasta:
    input:
        renamed_consensus=lambda wildcards: [
            os.path.join(output_dir, f"consensus/{sample}_r_{r}_s_{s}_con_renamed_{reference_name}.fas")
            for sample, reference_name in sample_reference_pairs
            for r in config["r"]
            for s in config["s"]
        ]
    output:
        os.path.join(output_dir, f"consensus/{run_name}.fasta")
    shell:
        """
        cat {input.renamed_consensus} > {output}
        """



#Create list of alignment files for stats
rule create_alignment_log:
    input:
        alignment_files=lambda wildcards: glob.glob(os.path.join(output_dir, "alignment/*_align_*.fas")),
        concat_cons=os.path.join(output_dir, f"consensus/{run_name}.fasta")
    output:
        alignment_log=os.path.join(output_dir, "alignment/alignment_files.log")
    run:
        alignment_files = input.alignment_files

        with open(output.alignment_log, 'w') as log_file:
            for file in alignment_files:
                log_file.write(f"{file}\n")





#Extract stats from alignment files using custom python script
rule extract_stats_to_csv:
    input:
        alignment_log=os.path.join(output_dir, "alignment/alignment_files.log"),
        out_files=expand(os.path.join(output_dir, "out/{sample}_r_{r}_s_{s}_summary_{reference_name}.out"),
                         zip, 
                         sample=[s for s, _ in sample_reference_pairs],
                         reference_name=[ref for _, ref in sample_reference_pairs],
                         r=config["r"],
                         s=config["s"]),
        concat_cons=os.path.join(output_dir, f"consensus/{run_name}.fasta")  
    output:
        stats=os.path.join(output_dir, "{run_name}.csv")
    params:
        script="path/to/mge_stats.py",
        out_dir=os.path.join(output_dir, "out/")
    shell:
        """
        python {params.script} {input.alignment_log} {output.stats} {params.out_dir}
        """




#Clean up superfluous files
rule cleanup_files:
    input:
        summary_csv=os.path.join(output_dir, f"{run_name}.csv")
    output:
        touch(os.path.join(output_dir, "cleanup_complete.txt"))
    run:
        #Remove all .log files in the logs directory
        log_dir = os.path.join(output_dir, "logs")

        log_files = glob.glob(os.path.join(log_dir, "*.log"))
        if log_files:
            for log_file in log_files:
                print(f"Removing log file: {log_file}")
                os.remove(log_file)
        
        with open(output[0], 'w') as f:
            f.write("Cleanup complete.")


