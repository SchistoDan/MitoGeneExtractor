import os
import csv
import yaml
from snakemake.io import expand
import pandas as pd
import glob


#Define and load config
configfile: "config-samplesCSV_genefetch.yaml"
config = yaml.safe_load(open("config-samplesCSV_genefetch.yaml"))



#Parse samples from .csv files
def parse_samples(samples_file):
    samples = {}
    with open(samples_file, mode='r') as infile:
        reader = csv.DictReader(infile)
        for row in reader:
            sample_id = row['ID']
            forward_read = row['forward']
            reverse_read = row['reverse']
            samples[sample_id] = {"R1": forward_read, "R2": reverse_read}
    return samples



#Parse protein references from CSV
def parse_protein_references(protein_reference_file):
    protein_references = {}
    with open(protein_reference_file, mode='r') as infile:
        reader = csv.DictReader(infile)
        for row in reader:
            sample_id = row['process_id']
            reference_name = row['reference_name']
            reference_path = row['reference_path']
            accession_number = row['accession_number']
            protein_references[sample_id] = {"reference_name": reference_name, "reference_path": reference_path}
    print(f"Protein references parsed: {protein_references}")
    return protein_references


samples = parse_samples(config["samples_file"])
protein_references = parse_protein_references(config["protein_reference_file"])
output_dir = config["output_dir"]




#Create lists of samples and corresponding references, and link them
sample_list = list(samples.keys())

reference_list = [protein_references[sample]["reference_name"] for sample in sample_list]

sample_reference_pairs = list(zip(sample_list, reference_list))


consensus_files = [
    os.path.join(output_dir, f"consensus/{sample}_con_{reference}.fas")
    for sample, reference in sample_reference_pairs
]



print(f"Output directory: {output_dir}")
print("sample reference pairs:", sample_reference_pairs)
print(f"Target consensus files: {consensus_files}")





rule all:
    input:
        os.path.join(output_dir, "summary_stats.csv")






#Rule to handle .fastq.gz files and clean headers
rule gunzip_and_clean_headers:
    input:
        R1=lambda wildcards: samples[wildcards.sample]["R1"],
        R2=lambda wildcards: samples[wildcards.sample]["R2"]
    output:
        R1_out=os.path.join(output_dir, "raw_data/{sample}_R1_clean.fastq"),
        R2_out=os.path.join(output_dir, "raw_data/{sample}_R2_clean.fastq")
    shell:
        """
        # Check if input is gzipped and decompress if needed
        if [[ {input.R1} == *.gz ]]; then
            gzip -cd {input.R1} > {output.R1_out}
        else
            cp {input.R1} {output.R1_out}
        fi

        if [[ {input.R2} == *.gz ]]; then
            gzip -cd {input.R2} > {output.R2_out}
        else
            cp {input.R2} {output.R2_out}
        fi
        
        sed -i 's/ /_/g' {output.R1_out}
        sed -i 's/ /_/g' {output.R2_out}
        """



	
#Run fastp for paired-end reads
rule fastp_pe:
    input:
        R1=os.path.join(output_dir, "raw_data/{sample}_R1_clean.fastq"),
        R2=os.path.join(output_dir, "raw_data/{sample}_R2_clean.fastq")
    output:
        R1_trimmed=os.path.join(output_dir, "trimmed_data/{sample}_R1_trimmed.fastq"),
        R2_trimmed=os.path.join(output_dir, "trimmed_data/{sample}_R2_trimmed.fastq"),
        report=os.path.join(output_dir, "trimmed_data/{sample}_fastp_report.html"),
        json=os.path.join(output_dir, "trimmed_data/{sample}_fastp_report.json")
    log:
        out=os.path.join(output_dir, "trimmed_data/{sample}_fastp.out"),
        err=os.path.join(output_dir, "trimmed_data/{sample}_fastp.err")
    shell:
        """
        fastp -i {input.R1} -I {input.R2} \
              -o {output.R1_trimmed} -O {output.R2_trimmed} \
              -a AGATCGGAAGAGCACACGTCTGAACTCCAGTCA \
              -A AGATCGGAAGAGCGTCGTGTAGGGAAAGAGTGT \
              --dedup \
              --trim_poly_g \
              -h {output.report} -j {output.json} \
              > {log.out} 2> {log.err}
        """




#Concatenate PE fastq files into a single file
rule fastq_concat:
    input:
        R1=os.path.join(output_dir, "trimmed_data/{sample}_R1_trimmed.fastq"),
        R2=os.path.join(output_dir, "trimmed_data/{sample}_R2_trimmed.fastq")
    output:
        temp(os.path.join(output_dir, "trimmed_data/{sample}_concat.fastq"))
    shell:
        """
        cat {input.R1} {input.R2} > {output}
        """




#Trim the concatenated fastq file
rule quality_trim:
    input:
        os.path.join(output_dir, "trimmed_data/{sample}_concat.fastq")
    output:
        os.path.join(output_dir, "trimmed_data/{sample}_concat_trimmed.fq")
    shell:
        """
        trim_galore --no_report_file --dont_gzip --output_dir {output_dir}/trimmed_data/ {input}
        """




#Run MitoGeneExtractor on trimmed fastq file
rule MitoGeneExtractor_default:
    input:
        DNA=os.path.join(output_dir, "trimmed_data/{sample}_concat_trimmed.fq"),
        AA=lambda wildcards: protein_references[wildcards.sample]["reference_path"]
    output:
        alignment=os.path.join(output_dir, "alignment/{sample}_align_{reference_name}.fas"),
        consensus=os.path.join(output_dir, "consensus/{sample}_con_{reference_name}.fas"),
        vulgar=os.path.join(output_dir, "logs/{sample}_vulgar_{reference_name}.txt")
    log:
        out=os.path.join(output_dir, "out/{sample}_summary_{reference_name}.out"),
        err=os.path.join(output_dir, "err/{sample}_summary_{reference_name}.err")
    shell:
        """
        /home/danip3/MitoGeneExtractor-main/MitoGeneExtractor-v1.9.5 \
        -q {input.DNA} -p {input.AA} \
        -o {output_dir}/alignment/{wildcards.sample}_align_ \
        -c {output_dir}/consensus/{wildcards.sample}_con_ \
        -V {output.vulgar}  \
        -n 0 -C 5 -r 1 -t 0.5 \
        > {log.out} 2> {log.err}
        """





#Concatenate all consensus fasta files
rule concatenate_fasta:
    input:
        consensus_files
    output:
        os.path.join(output_dir, "consensus/cox1_concatenated_consensus.fasta")
    shell:
        """
        cat {input} > {output}
        """


out_files = [
    os.path.join(output_dir, f"out/{sample}_summary_{reference}.out")
    for sample, reference in sample_reference_pairs
]

print(f"Target .out files: {out_files}")



#Extract data from .out files and generate summary_stats.csv
rule extract_data_to_csv:
    input:
        out_files=expand(os.path.join(output_dir, "out/{sample}_summary_{reference}.out"),
                         zip, sample=sample_list, reference=reference_list),
        fasta_file=os.path.join(output_dir, "consensus/cox1_concatenated_consensus.fasta")
    output:
        summary_csv=os.path.join(output_dir, "summary_stats.csv")
    run:
        data_list = []

        for out_file in input.out_files:
            with open(out_file, 'r') as file:
                data = {}
                for line in file:
                    terms = [
                        "consensus sequence output",
                        "Number of input sequences considered",
                        "Length of alignment",
                        "sequence found in vulgar file",
                        "number of aligned reads",
                        "Coverage minimum",
                        "Coverage maximum",
                        "Coverage mean",
                        "Coverage median",
                        "# skipped reads due to low rel. score"
                    ]
                    for term in terms:
                        if term in line:
                            key = term
                            value = line.split(term)[-1].strip().replace(":", "")
                            
                            #If the term is "consensus sequence output", extract only the final part of the path
                            if key == "consensus sequence output":
                                value = value.split('/')[-1]
                            
                            data[key] = value

            data_list.append(data)

        #Convert to df and save to csv file
        df = pd.DataFrame(data_list)

        df.to_csv(output.summary_csv, index=False)
