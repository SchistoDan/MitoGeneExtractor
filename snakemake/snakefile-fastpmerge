import os
import csv
import yaml
from snakemake.io import expand
import pandas as pd
import glob


#Define and load config
configfile: "config.yaml"
config = yaml.safe_load(open("config.yaml"))



#Parse samples from .csv files
def parse_samples(samples_file):
    samples = {}
    with open(samples_file, mode='r') as infile:
        reader = csv.DictReader(infile)
        for row in reader:
            sample_id = row['ID']
            forward_read = row['forward']
            reverse_read = row['reverse']
            samples[sample_id] = {"R1": forward_read, "R2": reverse_read}
    return samples



#Parse protein references from CSV
def parse_protein_references(protein_reference_file):
    protein_references = {}
    with open(protein_reference_file, mode='r') as infile:
        reader = csv.DictReader(infile)
        for row in reader:
            sample_id = row['process_id']
            reference_name = row['reference_name']
            reference_path = row['reference_path']
            accession_number = row['accession_number']
            protein_references[sample_id] = {"reference_name": reference_name, "reference_path": reference_path}
    print(f"Protein references parsed: {protein_references}")
    return protein_references


samples = parse_samples(config["samples_file"])
protein_references = parse_protein_references(config["protein_reference_file"])
output_dir = config["output_dir"]
run_name = config["run_name"]



#Create lists of samples and corresponding references, and link them
sample_list = list(samples.keys())
reference_list = [protein_references[sample]["reference_name"] for sample in sample_list]
sample_reference_pairs = list(zip(sample_list, reference_list))

consensus_files = [
    os.path.join(output_dir, f"consensus/{sample}_con_{reference}.fas")
    for sample, reference in sample_reference_pairs
]



print(f"Run name: {run_name}")
print(f"Output directory: {output_dir}")
print(f"r params: {r}")
print(f"s params: {s}")
print("reference list:", reference_list)
print("sample reference pairs:", sample_reference_pairs)




rule all:
    input:
        expand(
            os.path.join(output_dir, "consensus/{sample}_r_{r}_s_{s}_con_{reference_name}.fas"),
            zip,
            sample=[s for s, _ in sample_reference_pairs],
            reference_name=[ref for _, ref in sample_reference_pairs],
            r=config["r"],
            s=config["s"]
        ),
        os.path.join(output_dir, f"consensus/{run_name}.fasta"),
        os.path.join(output_dir, f"{run_name}.csv"), 
        os.path.join(output_dir, "cleanup_complete.txt")






#Run fastp for paired-end reads (dedup, adapter and poly g trim, and merge)
rule fastp_pe:
    input:
        R1=lambda wildcards: samples[wildcards.sample]["R1"],
        R2=lambda wildcards: samples[wildcards.sample]["R2"]
    output:
        R1_trimmed=os.path.join(output_dir, "trimmed_data/{sample}_R1_trimmed.fq.gz"),
        R2_trimmed=os.path.join(output_dir, "trimmed_data/{sample}_R2_trimmed.fq.gz"),
        report=os.path.join(output_dir, "trimmed_data/reports/{sample}_fastp_report.html"),
        json=os.path.join(output_dir, "trimmed_data/reports/{sample}_fastp_report.json"),
        merged_reads=os.path.join(output_dir, "trimmed_data/{sample}_merged.fq"),
        unpaired_R1=os.path.join(output_dir, "trimmed_data/unpaired/{sample}_unpaired_R1.fq"),
        unpaired_R2=os.path.join(output_dir, "trimmed_data/unpaired/{sample}_unpaired_R2.fq")
    log:
        err=os.path.join(output_dir, "trimmed_data/{sample}_fastp.err")
    shell:
        """
        fastp -i {input.R1} -I {input.R2} \
              -o {output.R1_trimmed} -O {output.R2_trimmed} \
              --adapter_sequence=AGATCGGAAGAGCACACGTCTGAACTCCAGTCA \
              --adapter_sequence_r2=AGATCGGAAGAGCGTCGTGTAGGGAAAGAGTGT \
              --dedup \
              --trim_poly_g \
              --merge --merged_out {output.merged_reads} \
              --unpaired1 {output.unpaired_R1} \
              --unpaired2 {output.unpaired_R2} \
              -h {output.report} -j {output.json} \
              > {log.err}
        """




#Rule to clean headers (sed whitespace)
rule clean_headers:
    input:
        merged_reads=os.path.join(output_dir, "trimmed_data/{sample}_merged.fq"),
        R1_unpaired=os.path.join(output_dir, "trimmed_data/unpaired/{sample}_unpaired_R1.fq"),
        R2_unpaired=os.path.join(output_dir, "trimmed_data/unpaired/{sample}_unpaired_R2.fq")
    output:
        clean_merged=os.path.join(output_dir, "trimmed_data/{sample}_merged_clean.fq"),
        clean_R1_unpaired=os.path.join(output_dir, "trimmed_data/unpaired/{sample}_unpaired_R1_clean.fq"),
        clean_R2_unpaired=os.path.join(output_dir, "trimmed_data/unpaired/{sample}_unpaired_R2_clean.fq")

    shell:
        """
        sed 's/ /_/g' {input.merged_reads} > {output.clean_merged}
        sed 's/ /_/g' {input.R1_unpaired} > {output.clean_R1_unpaired}
        sed 's/ /_/g' {input.R2_unpaired} > {output.clean_R2_unpaired}
        """






#Concatenate PE fastq files into a single file
rule concat:
    input:
        merged=os.path.join(output_dir, "trimmed_data/{sample}_merged_clean.fq"),
        R1=os.path.join(output_dir, "trimmed_data/unpaired/{sample}_unpaired_R1_clean.fq"),
        R2=os.path.join(output_dir, "trimmed_data/unpaired/{sample}_unpaired_R2_clean.fq")
    output:
        temp(os.path.join(output_dir, "trimmed_data/{sample}_concat.fastq"))
    shell:
        """
        cat {input.R1} {input.R2} {input.merged} > {output}
        """




#Run MitoGeneExtractor
rule MitoGeneExtractor_default:
    input:
        DNA=os.path.join(output_dir, "trimmed_data/{sample}_concat.fastq"),
        AA=lambda wildcards: protein_references[wildcards.sample]["reference_path"]
    output:
        alignment=os.path.join(output_dir, "alignment/{sample}_align_{reference_name}.fas"),
        consensus=os.path.join(output_dir, "consensus/{sample}_con_{reference_name}.fas"),
        vulgar=os.path.join(output_dir, "logs/{sample}_vulgar_{reference_name}.txt")
    log:
        out=os.path.join(output_dir, "out/{sample}_summary_{reference_name}.out"),
        err=os.path.join(output_dir, "err/{sample}_summary_{reference_name}.err")
    shell:
        """
        /home/danip3/MitoGeneExtractor-main/MitoGeneExtractor-v1.9.5 \
        -q {input.DNA} -p {input.AA} \
        -o {output_dir}/alignment/{wildcards.sample}_align_ \
        -c {output_dir}/consensus/{wildcards.sample}_con_ \
        -V {output.vulgar}  \
        -n 0 -C 5 -r 1 -t 0.5 \
        > {log.out} 2> {log.err}
        """




#Concatenate all .fas files in the consensus directory
rule concatenate_fasta:
    input:
        consensus_files
    output:
        os.path.join(output_dir, f"consensus/{run_name}.fasta")
    shell:
        """
        cat {input} > {output}
        """



#Extract data from .out files and generate a summary CSV
rule extract_data_to_csv:
    input:
        out_files=expand(os.path.join(output_dir, "out/{sample}_summary_{reference}.out"),
                         zip, sample=sample_list, reference=reference_list),
        fasta_file=os.path.join(output_dir, f"consensus/{run_name}.fasta")
    output:
        summary_csv=os.path.join(output_dir, "{run_name}.csv")
    run:
        data_list = []


        for out_file in input.out_files:
            
            if not os.path.isfile(out_file):
                print(f"File does not exist: {out_file}")
                continue

            with open(out_file, 'r') as file:
                data = {}
                for line in file:
                    terms = [
                        "consensus sequence output",
                        "Number of input sequences considered",
                        "Length of alignment",
                        "sequence found in vulgar file",
                        "number of aligned reads",
                        "Coverage minimum",
                        "Coverage maximum",
                        "Coverage mean",
                        "Coverage median",
                        "# skipped reads due to low rel. score"
                    ]

                    for term in terms:
                        if term in line:
                            key = term
                            value = line.split(term)[-1].strip().replace(":", "")
                            data[key] = value

            print(f"Extracted data from {out_file}: {data}")

            consensus_sequence_output = data.get("consensus sequence output", "").strip()
            consensus_sequence_output_filename = os.path.basename(consensus_sequence_output)

            consensus_sequence_output_filename = consensus_sequence_output_filename.replace("_con_", "")
            data['consensus sequence output'] = consensus_sequence_output_filename
            
            file_name = os.path.basename(out_file)
            parts = file_name.split("_")
            sample_name = "_".join(parts[0:4])  #E.g. BSNHM011-24_r_1.3_s_50
            reference_name = parts[-1].replace(".out", "")  #E.g. BSNHM011-24
            data['sample'] = sample_name
            data['reference'] = reference_name
            
            data_list.append(data)

        df = pd.DataFrame(data_list)

        seq_dict = {}

        with open(input.fasta_file, "r") as fasta_file:
            current_key = None
            for line in fasta_file:
                if line.startswith(">"):
                    if current_key:  # Save previous sequence data
                        seq = seq_dict.get(current_key, "")
                        total_length = len(seq)
                        num_n = seq.count('N')
                        num_gaps = seq.count('-') + seq.count('~')
                        seq_dict[current_key] = {
                            'total_length': total_length,
                            'num_n': num_n,
                            'num_gaps': num_gaps
                        }
                    #Strip 'Consensus_' prefix from fasta header for matching purposes
                    current_key = line.strip().lstrip(">").split()[0]
                    current_key = current_key.replace("Consensus_", "")  
                    print(f"Parsed FASTA header: {current_key}")  
                    seq_dict[current_key] = ""

                else:
                    if current_key:
                        seq_dict[current_key] += line.strip()

            if current_key:
                seq = seq_dict.get(current_key, "")
                total_length = len(seq)
                num_n = seq.count('N')
                num_gaps = seq.count('-') + seq.count('~')
                seq_dict[current_key] = {
                    'total_length': total_length,
                    'num_n': num_n,
                    'num_gaps': num_gaps
                }

        df['total_length'] = df['consensus sequence output'].map(lambda x: seq_dict.get(x, {}).get('total_length', None))
        df['num_n'] = df['consensus sequence output'].map(lambda x: seq_dict.get(x, {}).get('num_n', None))
        df['num_gaps'] = df['consensus sequence output'].map(lambda x: seq_dict.get(x, {}).get('num_gaps', None))

        df = df.drop(columns=['sample', 'reference'])

        df.to_csv(output.summary_csv, index=False)
        print(f"Summary statistics written to {output.summary_csv}")




#Clean up superfluous files
rule cleanup_files:
    input:
        summary_csv=os.path.join(output_dir, f"{run_name}.csv")
    output:
        touch(os.path.join(output_dir, "cleanup_complete.txt"))
    run:
        consensus_dir = os.path.join(output_dir, "consensus")
        log_dir = os.path.join(output_dir, "logs")


        # Remove all consensus files (except renamed ones)
        files_to_remove = [f for f in os.listdir(consensus_dir) 
                           if f.endswith(".fas") and "renamed" not in f]
        
        if files_to_remove:
            for file in files_to_remove:
                file_path = os.path.join(consensus_dir, file)
                print(f"Removing file: {file_path}")
                os.remove(file_path)

        # Remove all .log files in the logs directory
        log_files = glob.glob(os.path.join(log_dir, "*.log"))
        if log_files:
            for log_file in log_files:
                print(f"Removing log file: {log_file}")
                os.remove(log_file)
        
        with open(output[0], 'w') as f:
            f.write("Cleanup complete.")



